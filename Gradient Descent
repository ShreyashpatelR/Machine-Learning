import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def gradient_descent(x,y):
    m_curr = b_curr = 0
    iterations = 19000
    n = len(x)
    learning_rate = 0.01

    for i in range(iterations):
        y_predicted = m_curr * x + b_curr
        #cost = (1/n) * sum([val**2 for val in (y-y_predicted)])
        md = -(2/n)*sum(x*(y-y_predicted))
        bd = -(2/n)*sum(y-y_predicted)
        m_curr = m_curr - learning_rate * md
        b_curr = b_curr - learning_rate * bd
        #print ("m {}, b {} ".format(m_curr,b_curr))
        
    print ("m {}, b {} ".format(m_curr,b_curr))
    plt.scatter(x,y)
    Yp = m_curr*x + b_curr
    plt.plot(x,Yp,color='red')
    plt.show()
    
    Error = y-Yp
    SumError2 = Error**2
    print("Error  is ")
    print(np.sum(SumError2))
        

#x = np.array([2,3,5,13,8,16,11,1,9]) 
#y = np.array([15,28,42,64,50,90,58,8,54])
# from sklearn.datasets import load_boston
# boston_dataset = load_boston()
# boston= pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)
# boston.head()
# boston['MEDV']=boston_dataset.target
# boston.head()

# x= boston['RM']
# y = boston['MEDV']
x = np.array([1,2,4,6,8])
y= np.array([2,5,6,9,11])
Z = x+y
print(Z)
#print(x)

#plt.show()
gradient_descent(x,y)



#print ("m {}, b {}, cost {} iteration {}".format(m_curr,b_curr,cost, i))
